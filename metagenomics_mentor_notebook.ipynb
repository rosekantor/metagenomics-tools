{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ec5e40-53ac-4ba4-a69d-a0cd03f0760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5348bc1b-8a03-48e3-a845-b03ae10f3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list paths to software and locations we will need later\n",
    "idba_ud = '/shared/software/bin/idba_ud'\n",
    "mash = '/shared/software/bin/mash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0cbfe2-ccd4-4b4b-bd0e-bdeb7da38224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Hannah:\n",
    "# df = pd.read_csv('/Users/rosekantor/data/metagenomics_mentoring/arstagnation/quality_summary.tsv', sep='\\t')\n",
    "# df = df.rename(columns={'sample':'sample_id'})\n",
    "project = 'arstagnation'\n",
    "sample_id = ['ARSTAG_AR_4_27',\n",
    "             'ARSTAG_ARBF_12345_pre', 'ARSTAG_ARBF_1_post',\n",
    "             'ARSTAG_ARBF_2_post', 'ARSTAG_ARBF_3_post',\n",
    "             'ARSTAG_ARBF_4_post', 'ARSTAG_ARBF_5_post', \n",
    "             'ARSTAG_TAPRES_TAPRES_23', 'ARSTAG_TAPRES_TAPRES_27',\n",
    "             'ARSTAG_TAPRES_TAPRES_40', 'ARSTAG_TAPRES_TAPRES_41',\n",
    "             'ARSTAG_CONTROL_BFSLIDECONTROL_41', 'ARSTAG_CONTROL_MANIFB_41',\n",
    "             'ARSTAG_CONTROL_MOCK1E10_111821', 'ARSTAG_CONTROL_MOCK1E8_111821']\n",
    "df = pd.DataFrame()\n",
    "df['sample_id'] = sample_id\n",
    "\n",
    "# # Lauren:\n",
    "# project = 'dwdsf18'\n",
    "# df = pd.read_csv('/Users/rosekantor/data/metagenomics_mentoring/dwdsf18/sample_ids.txt', header=None, names=['sample_id'])\n",
    "# df['combined_reads'] = f'/groups/banfield/sequences/2022/' + df.sample_id + '/raw.d/' + df.sample_id + '_trim_clean.PE.fa'\n",
    "\n",
    "# # Jorien:\n",
    "# project = 'awtp2'\n",
    "# df = pd.read_csv('/Users/rosekantor/data/metagenomics_mentoring/awtp2/sample_info.csv')\n",
    "# df = df.rename(columns={'ggkbase_project_name':'sample_id'})\n",
    "# df['combined_reads'] = f'/groups/banfield/sequences/2022/' + df.sample_id + '/raw.d/' + df.sample_id + '_trim_clean.PE.fa'\n",
    "\n",
    "# assign paths\n",
    "assem_dir = f'/groups/banfield/projects/industrial/nelson_lab/{project}/assemblies'\n",
    "reads_dir = f'/groups/banfield/projects/industrial/nelson_lab/{project}/reads/trimmed'\n",
    "mash_dir = f'/groups/banfield/projects/industrial/nelson_lab/{project}/mash_analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12204566-f565-4717-a665-dd3435d26661",
   "metadata": {},
   "source": [
    "# Raw read processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5412f1-63fb-477a-ac6b-fa99bbbbbb5a",
   "metadata": {},
   "source": [
    "1. Run `bbmap` and `sickle` on raw reads (done for you).\n",
    "\n",
    "2. Make symlinks to reads in our analysis folders using `ln -s` so that we can have easy access to them in the future.\n",
    "\n",
    "3. Review the output of fastqc to check the quality of the reads.  If reads appear to be of low or questionable quality, run fastqc on the trimmed reads to check that quality of the remaining reads is higher after trimming and any concerning artifacts have been removed.\n",
    "\n",
    "4. Count the reads before and after trimming. From within /trimmed and /raw, open a tmux session and run this command for counting reads:\n",
    "    \n",
    "`seqkit stat *.fastq.gz > seqkit_output.tsv`\n",
    "\n",
    "5. Combine the sample project names table with the output table from seqkit to track the data for each sample.  How much total sequencing per sample, how many forward/reverse reads before/after trimming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996a369-4014-403a-b3dd-002244108c88",
   "metadata": {},
   "source": [
    "# Assembly commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696be85-879a-4207-95c1-717786daab5a",
   "metadata": {},
   "source": [
    "Using the sample table, generate commands for running assemblies on the cluster (via SLURM). Commands may specify whether to use the memory or high-memory nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "50c5dd34-723f-45bc-b8fb-080dd9fba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create an empty list, iterate thru the df and generate commands, save them in the list, then turn the list into a column in our df\n",
    "idba_cmd = []\n",
    "for row in df.itertuples():\n",
    "    # some samples may require more memory for assembly\n",
    "    if 'INF' in row.sample_id or 'BAC' in row.sample_id:\n",
    "        cmd = f'sbatch --partition memory --wrap \"' \\\n",
    "              f'{idba_ud} ' \\\n",
    "              f'--pre_correction -r {row.combined_reads} ' \\\n",
    "              f'-o {assem_dir}/{row.sample_id}.idba_ud\"'\n",
    "    else:\n",
    "        cmd = f'sbatch --wrap \"' \\\n",
    "          f'{idba_ud} ' \\\n",
    "          f'--pre_correction -r {row.combined_reads} ' \\\n",
    "          f'-o {assem_dir}/{row.sample_id}.idba_ud\"'\n",
    "\n",
    "    idba_cmd.append(cmd)\n",
    "df['idba_cmd'] = idba_cmd\n",
    "\n",
    "# write the commands to a CSV\n",
    "df['idba_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/1.assembly.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd4724-bdf9-4f92-b329-db9114c5a25f",
   "metadata": {},
   "source": [
    "note: alternatively you could just open a file and write the commands directly to the file as you create them (in the for loop). You could also open a jupyter notebook on biotite and run the commands directly from there (not recommended because it's harder to track what is running).\n",
    "\n",
    "Now push this shell script up to biotite and run it: `sh assembly.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d3c5-e336-4970-ab50-c9477a9a5fd2",
   "metadata": {},
   "source": [
    "# Mash commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766f307-08cc-4ec1-8271-e97332f54c38",
   "metadata": {},
   "source": [
    "Compare samples to each other using minhash distances between reads files.  First, need to combine forward and reverse reads into a single stream and then create a mash sketch for each sample.  Then all samples can be pairwise compared to each other to produce a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "01655242-e1f2-43e8-b186-b1d182d4df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mash all v. all reads\n",
    "mash_cmd = []\n",
    "for row in df.itertuples():\n",
    "    cmd = f'cat {reads_dir}/{s}_trim_clean.PE.1.fastq.gz {reads_dir}/{s}_trim_clean.PE.2.fastq.gz | {mash} sketch -m 2 -r - -I {row.sample_id} -s 10000 -o {mash_dir}/{row.sample_id}'\n",
    "    mash_cmd.append(cmd)\n",
    "    \n",
    "df['mash_cmd'] = mash_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "133c9484-5cad-448f-951a-b407a6c6cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['mash_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/2.mash.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68281cb-f04a-400f-a8a2-a28f08fa8971",
   "metadata": {},
   "source": [
    "## mash dist commands\n",
    "`mash paste` combines multiple sketches into a single sketch.  The first arg is output name, followed by a list of all the sketch files you want to combine\n",
    "\n",
    "Command: `mash paste awtp2.msh *msh`\n",
    "\n",
    "`mash dist` can sketch on the fly or take a sketch as input.  Because we are doing all-vs-all we use the same msh file as the query and reference.\n",
    "\n",
    "Command: `mash dist awtp2.msh awtp2.msh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7769096-6c90-4935-bffd-920a9d60b243",
   "metadata": {},
   "source": [
    "# Post-assembly commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ae0a6-f6c5-417c-8caf-19d926924ede",
   "metadata": {},
   "source": [
    "1. Rename the scaffolds so that they have the project name in the fasta headers.\n",
    "2. Check assembly quality using Itai Sharon's contig_stats.pl \n",
    "3. Filter scaffolds > 1000 bp into a new min1000.fa file for further analysis.\n",
    "4. Remove extra files created by the assembler\n",
    "5. Make bowtie2 indexes in their own folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3942f2a3-f5cb-42e8-a36e-d1477ef0ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "postassem_cmd = [] # create empty list\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "\n",
    "    # include \"sample_id\" in headers and rename file to sample_id_scaffold.fa\n",
    "    rehead = f\"sed 's/scaffold/{s}/g' {assem_dir}/{s}.idba_ud/scaffold.fa > {scaffolds}\"\n",
    "\n",
    "    # get contig stats\n",
    "    cstats = f'contig_stats.pl -i {scaffolds}'\n",
    "\n",
    "    # delete extra files from assembly\n",
    "\n",
    "    clean = f'rm {assem_dir}/{s}.idba_ud/kmer '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/contig-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/align-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/graph-* '\\\n",
    "            f'{assem_dir}/{s}.idba_ud/local-*'\n",
    "\n",
    "    # make directory to store bowtie2 indices in\n",
    "    mdbt2 = f'mkdir {assem_dir}/{s}.idba_ud/bt2/'\n",
    "\n",
    "    # index in prep for bowtie2 mapping to get true coverage for ggkbase\n",
    "    ind = f'bowtie2-build {scaffolds} {assem_dir}/{s}.idba_ud/bt2/{s}_scaffold.fa'\n",
    "    \n",
    "    cmd = [rehead, cstats, clean, mdbt2, ind]\n",
    "    all_cmd = '; '.join(cmd) # separate all commands by semicolon (so they will be executed in order for each sample)\n",
    "    postassem_cmd.append(all_cmd) # append command to list\n",
    "    \n",
    "df['postassem_cmd'] = postassem_cmd # add as a column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "34d3680f-996e-4f16-b00e-dee59fc6b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['postassem_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/3.postassem.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9e1a6-00d8-4e96-a5d6-ff7ea6cb1586",
   "metadata": {},
   "source": [
    "# ggkbase import: mapping and gene-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4c0a399-1a3f-40c0-9dc6-38154f24ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "\n",
    "self_map_cmd = []\n",
    "readcounts_cmd = []\n",
    "genecalls_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    \n",
    "    s = row.sample_id\n",
    "    bt2base = f'{assem_dir}/{s}.idba_ud/bt2/{s}_scaffold.fa'\n",
    "    r1 = f'{reads_dir}/{s}_trim_clean.PE.1.fastq.gz'\n",
    "    r2 = f'{reads_dir}/{s}_trim_clean.PE.2.fastq.gz'\n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "    scaffolds_min1000 = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa'\n",
    "\n",
    "    # perform self-mapping and add readcounts to scaffold headers\n",
    "    map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} 2> {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.log | {shrinksam} -v > {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.sam\"'\n",
    "    self_map_cmd.append(map_cmd)\n",
    "    \n",
    "    add_readcount = f'/groups/banfield/software/pipeline/v1.1/scripts/add_read_count.rb {assem_dir}/{s}.idba_ud/{s}_scaffold_mapped.sam {assem_dir}/{s}.idba_ud/{s}_scaffold.fa 150 > {assem_dir}/{s}.idba_ud/{s}_scaffold.fa.counted'\n",
    "    move_file = f'mv {assem_dir}/{s}.idba_ud/{s}_scaffold.fa.counted {assem_dir}/{s}.idba_ud/{s}_scaffold.fa'\n",
    "    \n",
    "    # filter for only contigs ≥1000 bp\n",
    "    min1000 = f'pullseq -i {scaffolds} --min 1000 > {scaffolds_min1000}'\n",
    "    \n",
    "    readcounts_cmd.append(f'{add_readcount}; {move_file}; {min1000}') # append command to list\n",
    "    \n",
    "    # gene-calling\n",
    "    call_orfs = f'prodigal -i {scaffolds_min1000} -o {scaffolds_min1000}.genes -a {scaffolds_min1000}.genes.faa -d {scaffolds_min1000}.genes.fna -m -p meta'\n",
    "    call_16S = f'/groups/banfield/software/pipeline/v1.1/scripts/16s.sh {scaffolds_min1000} > {scaffolds_min1000}.16s'\n",
    "    call_trnas = f'/groups/banfield/software/pipeline/v1.1/scripts/trnascan_pusher.rb -i {scaffolds_min1000} > /dev/null 2>&1'\n",
    "    genecalls_cmd.append(f'{call_orfs}; {call_16S}; {call_trnas}')\n",
    "\n",
    "df['self_map_cmd'] = self_map_cmd\n",
    "df['readcounts_cmd'] = readcounts_cmd\n",
    "df['genecalls_cmd'] = genecalls_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "568d4dd6-4a37-4252-8599-11cd34e2e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['self_map_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/4.ggkbase_self_map.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['readcounts_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/5.ggkbase_readcounts.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['genecalls_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/6.ggkbase_genecalls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20949e-2512-47ef-853c-46575e1c5f8d",
   "metadata": {},
   "source": [
    "# ggkbase: Annotation by best reciprocal blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91fab103-b92f-4aaf-ace2-3a07fdaca51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "usearch = '/groups/banfield/software/pipeline/v1.1/scripts/cluster_usearch_wrev.rb'\n",
    "annolookup = '/shared/software/bin/annolookup.py'\n",
    "\n",
    "usearch_cmds = []\n",
    "anno_proccess_cmds = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    faa = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa.genes.faa'\n",
    "    \n",
    "    # make usearch commands (using cluster usearch wrapper)\n",
    "    search_kegg = f'sbatch --wrap \"{usearch} -i {faa} -k -d kegg --nocluster\"'\n",
    "    search_uni = f'sbatch --wrap \"{usearch} -i {faa} -k -d uni --nocluster\"'\n",
    "    search_uniprot = f'sbatch --wrap \"{usearch} -i {faa} -k -d uniprot --nocluster\"'\n",
    "    \n",
    "    usearch_cmds.append(f'{search_kegg}; {search_uni}; {search_uniprot}')\n",
    "\n",
    "    # gzip files\n",
    "    gzip_b6 = f'gzip {faa}*.b6'\n",
    "    \n",
    "    # annolookup\n",
    "    anno_kegg = f'{annolookup} {faa}-vs-kegg.b6.gz kegg > {faa}-vs-kegg.b6+'\n",
    "    anno_uni = f'{annolookup} {faa}-vs-uni.b6.gz uniref > {faa}-vs-uni.b6+'\n",
    "    anno_uniprot = f'{annolookup} {faa}-vs-uniprot.b6.gz uniprot > {faa}-vs-uniprot.b6+'\n",
    "    \n",
    "    anno_proccess_cmds.append(f'{gzip_b6}; {anno_kegg}; {anno_uni}; {anno_uniprot}')\n",
    "\n",
    "df['usearch_cmds'] = usearch_cmds\n",
    "df['anno_proccess_cmds'] = anno_proccess_cmds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3a5583d-2aaf-4dd2-a58e-299b3d9e576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the commands to a CSV\n",
    "df['usearch_cmds'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/7.ggkbase_usearch.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['anno_proccess_cmds'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/8.ggkbase_anno_process.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07675f8a-6a66-4376-9419-4c0591c6fa8c",
   "metadata": {},
   "source": [
    "The files have now been generated for ggkbase import."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed043f-f3b6-47bb-8e1d-b941780ead24",
   "metadata": {},
   "source": [
    "# Anvi'o process contigs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8b233-f9f6-4083-9826-e14ec372403b",
   "metadata": {},
   "source": [
    "See https://merenlab.org/2016/06/18/importing-taxonomy/ and https://merenlab.org/2016/06/22/anvio-tutorial-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c213b4d-e4c4-464d-bd8d-f16ec5836be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "awk = \"awk '{print $1}'\" # using this to remove additional fields from fasta headers for contigs, ignore if you don't have this issue\n",
    "anvio = 'conda activate anvio-7.1' # note that this didn't actually work, needed to use conda run but we used system anvio v7\n",
    "\n",
    "kaiju_path = '/groups/banfield/projects/industrial/nelson_lab/kaiju/bin'\n",
    "kaiju_nodes = '/shared/db/kaiju/nr_euk/r2021-02-24/nodes.dmp'\n",
    "kaiju_names = '/shared/db/kaiju/nr_euk/r2021-02-24/names.dmp'\n",
    "\n",
    "kaiju_nr = '/shared/db/kaiju/nr_euk/r2021-02-24/kaiju_db_nr_euk.fmi'\n",
    "kaiju = f'{kaiju_path}/kaiju -t {kaiju_nodes} -f {kaiju_nr}'\n",
    "kaiju_addtaxnames = f'{kaiju_path}/kaiju-addTaxonNames -t {kaiju_nodes} -n {kaiju_names} -r superkingdom,phylum,order,class,family,genus,species'\n",
    "\n",
    "fix_scaffolds_cmd = []\n",
    "analyze_contigs_cmd = []\n",
    "addTaxonomy_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    # define file names\n",
    "    scaffolds_badheaders = f'{assem_dir}/{s}.idba_ud/{s}_scaffold_min1000.fa'\n",
    "    scaffolds_min1000 = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "    gene_calls = f'{assem_dir}/{s}.idba_ud/{s}_gene_calls.fa'\n",
    "    kaiju_out = f'{assem_dir}/{s}.idba_ud/{s}_kaiju.out'\n",
    "    kaiju_processed = f'{assem_dir}/{s}.idba_ud/{s}_genes_kaiju.txt'\n",
    "    \n",
    "    # fix headers\n",
    "    cmd = f'{awk} {scaffolds_badheaders} > {scaffolds_min1000}'\n",
    "    fix_scaffolds_cmd.append(cmd)\n",
    "    \n",
    "    # write commands for contigs analysis on cluster\n",
    "    make_cdb = f'anvi-gen-contigs-database -f {scaffolds_min1000} -o {contigsDB} -n {s} -T 48'\n",
    "    get_genes = f'anvi-get-sequences-for-gene-calls -c {contigsDB} -o {gene_calls}'\n",
    "    anvhmms = f'anvi-run-hmms -c {contigsDB} -T 48'\n",
    "    anvscg = f'anvi-run-scg-taxonomy -c {contigsDB} -T 48' # this didn't actually run because it wasn't set up yet!\n",
    "    #anvcogs = f'/shared/software/bin/anvi-run-ncbi-cogs -c {contigsDB} -T 48'\n",
    "    run_kaiju = f'{kaiju} -i {gene_calls} -v -z 48 > {kaiju_out}' \n",
    "    cmd = f'sbatch --wrap \"{anvio}; {make_cdb}; {get_genes}; {anvhmms}; {anvscg}; {run_kaiju}\"'\n",
    "    analyze_contigs_cmd.append(cmd)\n",
    "    \n",
    "    ## write commands for local jobs after cluster jobs\n",
    "    process_kaiju = f'{kaiju_addtaxnames} -i {kaiju_out} -o {kaiju_processed}' \n",
    "    import_kaiju = f'anvi-import-taxonomy-for-genes -i {kaiju_processed} -c {contigsDB} -p kaiju --just-do-it'\n",
    "    cmd = f'{process_kaiju}; {import_kaiju}'\n",
    "    addTaxonomy_cmd.append(cmd)\n",
    "\n",
    "# add to dataframe\n",
    "df['fix_scaffolds_cmd'] = fix_scaffolds_cmd\n",
    "df['analyze_contigs_cmd'] = analyze_contigs_cmd\n",
    "df['addTaxonomy_cmd'] = addTaxonomy_cmd\n",
    "\n",
    "# save files\n",
    "df['fix_scaffolds_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_0-fix_scaffold_names.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['analyze_contigs_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_1-analyze_contigs.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "df['addTaxonomy_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_2-addTaxonomy.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5c8c5-34d5-4a13-b9c6-009cd5a7c4a2",
   "metadata": {},
   "source": [
    "you must run `conda activate anvio-7.1` in terminal before running the addTaxonomy commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f81c92fd-8fd4-423a-b093-4c0352ad1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run-scg failed because set-up-scg hadn't been run so rerunning\n",
    "\n",
    "scg_taxonomy_cmd = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    s = row.sample_id\n",
    "    \n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "\n",
    "    anvscg = f'anvi-run-scg-taxonomy -c {contigsDB} -T 16' # this didn't actually run because it wasn't set up yet!\n",
    "    scg_taxonomy_cmd.append(anvscg)\n",
    "\n",
    "df['scg_taxonomy_cmd'] = scg_taxonomy_cmd\n",
    "\n",
    "# save files\n",
    "df['scg_taxonomy_cmd'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_scg_taxonomy_cmd.sh', index=False, quoting=csv.QUOTE_NONE, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dbd24-8c11-4099-b6ba-ed6867ff3f57",
   "metadata": {},
   "source": [
    "# Cross-mapping reads and anvio profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5139ab4-9260-4f57-abde-847888cdd113",
   "metadata": {},
   "source": [
    "Build bt2 directories for our min1000 scaffolds to match the Anvio contigs db scaffolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e05a3f3-6d91-4a12-b156-90042adb24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_3-bt2build.sh', 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "        bt2ind = f'{assem_dir}/{s}.idba_ud/bt2/{s}_min1000.fa'\n",
    "        # index in prep for bowtie2 mapping to get coverage on min1000 scaffolds\n",
    "        f.write(f'bowtie2-build {scaffolds} {bt2ind}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "59f9f16d-a121-44cf-9a40-7535bb853aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinksam = '/shared/software/bin/shrinksam'\n",
    "bt2 = '/shared/software/bin/bowtie2'\n",
    "anvio = 'conda run -n anvio-7.1'\n",
    "\n",
    "xmapping_df = []\n",
    "for srow in df.itertuples():  \n",
    "    s = srow.sample_id\n",
    "\n",
    "    scaffolds = f'{assem_dir}/{s}.idba_ud/{s}_min1000.fa'\n",
    "    bt2base = f'{assem_dir}/{s}.idba_ud/bt2/{s}_min1000.fa'\n",
    "    contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "    \n",
    "    # map against other samples and controls\n",
    "    for rrow in df.itertuples():\n",
    "        r = rrow.sample_id\n",
    "        r_fixed = re.sub('[\\.-]', '_', r) # if your sample name has dashes or periods in it, Anvio won't like it.\n",
    "        \n",
    "        r1 = f'{reads_dir}/{r}_trim_clean.PE.1.fastq.gz'\n",
    "        r2 = f'{reads_dir}/{r}_trim_clean.PE.2.fastq.gz'        \n",
    "        raw_bam = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}-raw.bam'\n",
    "        filtered_bam_raw = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}-raw-filt.bam'\n",
    "        bam = f'{assem_dir}/{s}.idba_ud/{s}-vs-{r}.bam'\n",
    "        \n",
    "        if 'CTRL' in r or 'CONTROL' in r:\n",
    "            \n",
    "            # filter mapping - for decontamination, we want to use filtered mappings\n",
    "            map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {raw_bam}; ' \\\n",
    "                      f'reformat.sh in={raw_bam} out={filtered_bam_raw} editfilter=2 threads=48; rm {raw_bam}\"'\n",
    "        \n",
    "        else:\n",
    "            map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {filtered_bam_raw}\"'\n",
    "        \n",
    "        # map_cmd = f'sbatch --wrap \"{bt2} -p 48 -x {bt2base} -1 {r1} -2 {r2} | {shrinksam} -v | sambam > {filtered_bam_raw}\"'\n",
    "        \n",
    "        # process mapping\n",
    "        initbam = f'anvi-init-bam {filtered_bam_raw} -o {bam}; rm {filtered_bam_raw}'\n",
    "        \n",
    "        # anvi-profile\n",
    "        profile_out = f'{assem_dir}/{s}.idba_ud/anvio_data/{r}_profile' # check this\n",
    "        anvip_cmd = f'sbatch --wrap \"{anvio} anvi-profile --min-contig-length 1000 --skip-SNV-profiling -T 48 -i {bam} -c {contigsDB} -o {profile_out} -S {r_fixed}\"'\n",
    "        \n",
    "        ## append all commands to table\n",
    "        xmapping_df.append([s, r, map_cmd, initbam, anvip_cmd])\n",
    "xmapping_df = pd.DataFrame.from_records(xmapping_df, columns=['assembly', 'reads', 'map', 'initbam', 'anvi-profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76e19f8d-c2e4-4e25-9332-79150c4a57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmapping_df.to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/crossmapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a7e65b0-c157-4920-9d75-796d539ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of filtering the xmapping_df to get just the rows you want\n",
    "pos = xmapping_df[(xmapping_df.assembly == 'AWTP2_CTRL_POS-POWER9_EXTRACTION_0') \n",
    "                  & ((xmapping_df.reads.str.contains('POS')) | (xmapping_df.reads.str.contains('BLANK')))]\n",
    "\n",
    "pos['map'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_xmap_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "pos['initbam'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_initbam_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)\n",
    "pos['anvi-profile'].to_csv(f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_anvip_pos-controls.sh', index=False, quoting=csv.QUOTE_NONE, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae829929-53fe-43f9-a7f3-cebfa4b0b673",
   "metadata": {},
   "source": [
    "Based on MASH distances between samples, decide which samples to cross-map against one another. A rule of thumb is that cross-mapping beyond 12 samples likely will not improve your bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f03f1-f869-4b86-a6c8-1f1f49939102",
   "metadata": {},
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcbff81-63f8-48f1-b3be-fce5ee71b18f",
   "metadata": {},
   "source": [
    "Note: you may want to tell concoct how many bins to create based on the counts of the SCGs in your samples.\n",
    "\n",
    "To do this, you could add a column to your df with that information and then uncomment the lines below (and include in the concoct command)\n",
    "\n",
    "One of the Anvi'o tutorials suggests it's better to underestimate the number of genomes since it is easier to split contaminated bins with anvi-refine than to combine fragmented bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e257b9e-5526-4460-b26a-265e5200d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f'/Users/rosekantor/data/metagenomics_mentoring/{project}/anvio_concoct.sh'\n",
    "with open(outfile, 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        s = row.sample_id\n",
    "        # c = row.scg_count\n",
    "        profileDB = f'{assem_dir}/{s}.idba_ud/anvio_data/merged/PROFILE.db'\n",
    "        contigsDB = f'{assem_dir}/{s}.idba_ud/{s}_contigs.db'\n",
    "        out = f'{assem_dir}/{s}.idba_ud/anvio_data'\n",
    "\n",
    "        get_cococt_input = f'anvi-export-splits-and-coverages -p {profileDB} -c {contigsDB} -o {out} -O {s} --splits-mode'\n",
    "        run_concoct = f'concoct --coverage_file {out}/{s}-COVs.txt --composition_file {out}/{s}-SPLITS.fa -b {out}/{s}_concoct -r 150 -t 10' #-c {c}\n",
    "        csv_to_tsv = f'sed \"s/,/\\\\t/g\" {out}/{s}_concoct_clustering_gt1000.csv > {out}/{s}_concoct_clustering_gt1000.tsv'\n",
    "        import_collection = f'anvi-import-collection {s}_concoct_clustering_gt1000.tsv -p {profileDB} -c {contigsDB} -C concoct'\n",
    "\n",
    "        f.write(f'{get_cococt_input}; {run_concoct}; {csv_to_tsv}; {import_collection}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b690339-1c38-4b95-96f7-edd568db083e",
   "metadata": {},
   "source": [
    "# using regular expressions to parse file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1f8e62a-5e65-450e-888e-bd1e9b842339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat = r'Total number of sequences: (\\d+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21f86794-fe3b-4c9d-8ffc-80d812c2cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences: 78893\n",
      "\n",
      "('78893',)\n"
     ]
    }
   ],
   "source": [
    "file = '/Users/rosekantor/data/metagenomics_mentoring/ARSTAG/ARSTAG_AR_1_23_scaffold.fa.summary.txt'\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    for line in f:\n",
    "        if 'Total number of sequences' in line:\n",
    "            scaffold_count = re.match(pat, line).groups(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf597fc-b4f0-4ca7-925b-0f76da01d57c",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Post-assembly\n",
    "- assess assembly quality, consider/test coassemblies\n",
    "\n",
    "Prep for ggkbase:\n",
    "- map to get true coverage for ggkbase\n",
    "- gene calls for ggkbase\n",
    "- annotation for ggkbase\n",
    "- import into ggkbase projects\n",
    "\n",
    "Prep for anvio:\n",
    "- anvio gen-contigs-db and gen-profile-db\n",
    "- kaiju for taxonomy and import into anvio\n",
    "- import annotations (from ggkbase) into anvio\n",
    "\n",
    "Binning:\n",
    "- cross-mapping for binning\n",
    "- auto-bin with DasTool\n",
    "- refine bins with ggkbase or anvio or both\n",
    "- evaluate bins with checkM2\n",
    "- dereplicate bins across samples with dRep\n",
    "- decontaminate assemblies using negative controls (also decontaminate reads)\n",
    "\n",
    "Additional questions:\n",
    "- annotate with special HMMs (e.g. ARGs, Metabolic)\n",
    "- investigate metabolisms of key organisms of interest\n",
    "- Virsorter2 to look for phage\n",
    "- look for pathogens\n",
    "- phylogenetic trees for key organisms of interest\n",
    "- assess growth via iRep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
